# -*- coding: utf-8 -*-
"""LoadForecast(Claude8models).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C6YZ1jNhOgGsjpWiv9tXEcQqV5wxvToN
"""

!pip install catboost -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Deep Learning Libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, ConvLSTM2D, Flatten, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Time Series Libraries
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Gradient Boosting Libraries
import xgboost as xgb
import catboost as cb

# Data manipulation
import zipfile
import requests
from io import BytesIO
import os
from io import StringIO

#Saving
from joblib import dump

class ElectricityLoadForecaster:
    def __init__(self):
        self.models = {}
        self.predictions = {}
        self.metrics = {}
        self.scaler = MinMaxScaler()
        self.data = None

    def load_and_preprocess_data(self, file_name=None):
        """Load and preprocess the electricity consumption data"""

        if file_name is None:
            print("Please download the dataset from: https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption")
            print("For this example, I'll create synthetic data with similar characteristics")
            # Create synthetic data
            dates = pd.date_range(start='2006-12-16', end='2010-11-26', freq='min')
            np.random.seed(42)

            # Create realistic electricity consumption pattern
            base_consumption = 2.0
            daily_pattern = np.sin(2 * np.pi * np.arange(len(dates)) / (24 * 60)) * 0.5
            weekly_pattern = np.sin(2 * np.pi * np.arange(len(dates)) / (7 * 24 * 60)) * 0.3
            noise = np.random.normal(0, 0.2, len(dates))
            trend = np.linspace(0, 0.5, len(dates))

            global_active_power = base_consumption + daily_pattern + weekly_pattern + trend + noise
            global_active_power = np.maximum(global_active_power, 0.1)  # Ensure positive values

            self.data = pd.DataFrame({
                'datetime': dates,
                'Global_active_power': global_active_power
            })
        else:

            self.data = pd.read_csv(file_name, sep=';', low_memory=False)#, parse_dates=['Date','Time'])
            self.data['datetime'] = pd.to_datetime(self.data['Date'] + ' ' + self.data['Time'], errors='coerce')

        # Clean data
        self.data = self.data.dropna()
        self.data.set_index('datetime', inplace=True)
        self.data.drop(['Date', 'Time'], axis=1, inplace=True)
        self.data = self.data.apply(pd.to_numeric)

        # Resample to hourly data for computational efficiency
        self.data = self.data.resample('H').mean()
        # Fill missing values after resampling:
        self.data = self.data.interpolate(method='linear')

        print(f"Data shape: {self.data.shape}")
        print(f"Date range: {self.data.index.min()} to {self.data.index.max()}")

        return self.data

    def create_sequences(self, data, seq_length, target_col='Global_active_power'):
        """Create sequences for time series models"""
        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i:(i + seq_length)])
            y.append(data[i + seq_length])
        return np.array(X), np.array(y)

    def prepare_data_splits(self, test_size=0.2, val_size=0.1, seq_length=24):
        """Prepare train, validation, and test splits"""
        # Scale the data
        scaled_data = self.scaler.fit_transform(self.data[['Global_active_power']])

        # Split data
        train_size = int(len(scaled_data) * (1 - test_size - val_size))
        val_size_abs = int(len(scaled_data) * val_size)

        train_data = scaled_data[:train_size]
        val_data = scaled_data[train_size:train_size + val_size_abs]
        test_data = scaled_data[train_size + val_size_abs:]

        # Create sequences for deep learning models
        X_train, y_train = self.create_sequences(train_data.flatten(), seq_length)
        X_val, y_val = self.create_sequences(val_data.flatten(), seq_length)
        X_test, y_test = self.create_sequences(test_data.flatten(), seq_length)

        print('train:', len(X_train), len(y_train))
        return {
            'train': (X_train, y_train),
            'val': (X_val, y_val),
            'test': (X_test, y_test),
            'train_data': train_data,
            'val_data': val_data,
            'test_data': test_data
        }

    def train_linear_regression(self, data_splits):
        """Train Linear Regression model"""
        print("Training Linear Regression...")
        X_train, y_train = data_splits['train']
        X_test, y_test = data_splits['test']

        # Reshape for sklearn
        X_train_lr = X_train.reshape(X_train.shape[0], -1)
        X_test_lr = X_test.reshape(X_test.shape[0], -1)

        model = LinearRegression()
        model.fit(X_train_lr, y_train)

        predictions = model.predict(X_test_lr)
        self.models['Linear_Regression'] = model
        self.predictions['Linear_Regression'] = predictions

        return predictions

    def train_arima(self, data_splits):
        """Train ARIMA model"""
        print("Training ARIMA...")
        train_data = data_splits['train_data'].flatten()
        test_data = data_splits['test_data'].flatten()

        # Fit ARIMA model
        model = ARIMA(train_data, order=(2, 1, 2))
        fitted_model = model.fit()

        # Forecast
        predictions = fitted_model.forecast(steps=len(test_data))

        self.models['ARIMA'] = fitted_model
        self.predictions['ARIMA'] = predictions

        return predictions

    def train_lstm(self, data_splits):
        """Train LSTM model"""
        print("Training LSTM...")
        X_train, y_train = data_splits['train']
        X_val, y_val = data_splits['val']
        X_test, y_test = data_splits['test']

        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
            Dropout(0.2),
            LSTM(50, return_sequences=False),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])

        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

        model.fit(X_train, y_train, epochs=50, batch_size=32,
                 validation_data=(X_val, y_val), callbacks=[early_stop], verbose=0)

        predictions = model.predict(X_test)

        self.models['LSTM'] = model
        self.predictions['LSTM'] = predictions.flatten()

        return predictions.flatten()

    def train_gru(self, data_splits):
        """Train GRU model"""
        print("Training GRU...")
        X_train, y_train = data_splits['train']
        X_val, y_val = data_splits['val']
        X_test, y_test = data_splits['test']

        model = Sequential([
            GRU(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
            Dropout(0.2),
            GRU(50, return_sequences=False),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])

        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

        model.fit(X_train, y_train, epochs=50, batch_size=32,
                 validation_data=(X_val, y_val), callbacks=[early_stop], verbose=0)

        predictions = model.predict(X_test)

        self.models['GRU'] = model
        self.predictions['GRU'] = predictions.flatten()

        return predictions.flatten()

    def train_xgboost(self, data_splits):
        """Train XGBoost model"""
        print("Training XGBoost...")
        X_train, y_train = data_splits['train']
        X_test, y_test = data_splits['test']

        # Reshape for XGBoost
        X_train_xgb = X_train.reshape(X_train.shape[0], -1)
        X_test_xgb = X_test.reshape(X_test.shape[0], -1)

        model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )

        model.fit(X_train_xgb, y_train)
        predictions = model.predict(X_test_xgb)

        self.models['XGBoost'] = model
        self.predictions['XGBoost'] = predictions

        return predictions

    def train_catboost(self, data_splits):
        """Train CatBoost model"""
        print("Training CatBoost...")
        X_train, y_train = data_splits['train']
        X_test, y_test = data_splits['test']

        # Reshape for CatBoost
        X_train_cb = X_train.reshape(X_train.shape[0], -1)
        X_test_cb = X_test.reshape(X_test.shape[0], -1)

        model = cb.CatBoostRegressor(
            iterations=100,
            learning_rate=0.1,
            depth=6,
            random_seed=42,
            verbose=False
        )

        model.fit(X_train_cb, y_train)
        predictions = model.predict(X_test_cb)

        self.models['CatBoost'] = model
        self.predictions['CatBoost'] = predictions

        return predictions

    def train_conv_lstm(self, data_splits):
        """Train ConvLSTM model"""
        print("Training ConvLSTM...")
        X_train, y_train = data_splits['train']
        X_val, y_val = data_splits['val']
        X_test, y_test = data_splits['test']

        # Reshape for ConvLSTM (samples, time_steps, rows, cols, channels)
        # We'll treat the sequence as a 1D convolution
        X_train_conv = X_train.reshape(X_train.shape[0], X_train.shape[1], 1, 1, 1)
        X_val_conv = X_val.reshape(X_val.shape[0], X_val.shape[1], 1, 1, 1)
        X_test_conv = X_test.reshape(X_test.shape[0], X_test.shape[1], 1, 1, 1)

        model = Sequential([
            ConvLSTM2D(filters=32, kernel_size=(1, 1),
                      input_shape=(X_train.shape[1], 1, 1, 1),
                      return_sequences=True),
            Dropout(0.2),
            ConvLSTM2D(filters=16, kernel_size=(1, 1)),
            Dropout(0.2),
            Flatten(),
            Dense(50),
            Dense(1)
        ])

        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

        model.fit(X_train_conv, y_train, epochs=30, batch_size=32,
                 validation_data=(X_val_conv, y_val), callbacks=[early_stop], verbose=0)

        predictions = model.predict(X_test_conv)

        self.models['ConvLSTM'] = model
        self.predictions['ConvLSTM'] = predictions.flatten()

        return predictions.flatten()

    def calculate_metrics(self, y_true, y_pred):
        """Calculate evaluation metrics"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        r2 = r2_score(y_true, y_pred)

        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'R2': r2
        }

    def train_all_models(self):
        """Train all models and compare performance"""
        # Prepare data
        data_splits = self.prepare_data_splits()
        y_test = data_splits['test'][1]

        # Train all models
        self.train_linear_regression(data_splits)
        self.train_arima(data_splits)
        self.train_lstm(data_splits)
        self.train_gru(data_splits)
        self.train_xgboost(data_splits)
        self.train_catboost(data_splits)
        self.train_conv_lstm(data_splits)
        #self.train_arima_lstm(data_splits)

        # Calculate metrics for all models
        for model_name, predictions in self.predictions.items():
            if model_name == 'ARIMA':
                # ARIMA predictions are for the entire test period
                y_true = data_splits['test_data'].flatten()
            else:
                y_true = y_test

            # Ensure predictions and true values have the same length
            min_len = min(len(y_true), len(predictions))
            y_true_cut = y_true[:min_len]
            pred_cut = predictions[:min_len]

            self.metrics[model_name] = self.calculate_metrics(y_true_cut, pred_cut)

    def plot_results(self):
        """Plot comparison of all models"""
        # Create metrics comparison
        metrics_df = pd.DataFrame(self.metrics).T

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()

        metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2']

        for i, metric in enumerate(metrics_to_plot):
            if i < len(axes):
                metrics_df[metric].plot(kind='bar', ax=axes[i], title=f'{metric} Comparison')
                axes[i].tick_params(axis='x', rotation=45)
                axes[i].grid(True, alpha=0.3)

        # Plot predictions comparison (sample of first 100 points)
        ax = axes[5]
        sample_size = min(100, len(list(self.predictions.values())[0]))

        for model_name, predictions in self.predictions.items():
            if model_name != 'ARIMA':  # ARIMA has different length
                ax.plot(predictions[:sample_size], label=model_name, alpha=0.7)

        ax.set_title('Predictions Comparison (First 100 Points)')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        return metrics_df

    def get_best_model(self):
        """Return the best performing model based on RMSE"""
        best_model = min(self.metrics.items(), key=lambda x: x[1]['RMSE'])
        return best_model[0], best_model[1]

# Usage example
def main():
    # Initialize forecaster
    global forecaster
    forecaster = ElectricityLoadForecaster()

    # Load and preprocess data
    #!wget -O household_power.zip 'https://archive.ics.uci.edu/static/public/235/individual+household+electric+power+consumption.zip'
    #!unzip household_power.zip
    data = forecaster.load_and_preprocess_data(file_name='household_power_consumption.txt')

    print("Starting model training and comparison...")

    # Train all models
    forecaster.train_all_models()

    # Display results
    print("\n" + "="*50)
    print("MODEL COMPARISON RESULTS")
    print("="*50)

    metrics_df = forecaster.plot_results()
    print("\nDetailed Metrics:")
    print(metrics_df.round(4))

    # Get best model
    best_model_name, best_metrics = forecaster.get_best_model()
    print(f"\nBest Model: {best_model_name}")
    print(f"Best RMSE: {best_metrics['RMSE']:.4f}")

    dump(forecaster.models[best_model_name], 'best_model.pkl')

    return forecaster

if __name__ == "__main__":
    forecaster = main()

print(forecaster.get_best_model()[0])